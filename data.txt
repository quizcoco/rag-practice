검색 증강 생성(Retrieval-augmented generation, RAG)은 대형 언어 모델 (LLM)이 새로운 정보를 검색하고 통합할 수 있도록 하는 기술이다. RAG를 사용하면 LLM은 지정된 문서 집합을 참조할 때까지 사용자 쿼리에 응답하지 않는다. 이 문서들은 LLM의 기존 훈련 데이터의 정보를 보완한다. 이를 통해 LLM은 훈련 데이터에서 사용할 수 없는 도메인 특정 및 업데이트된 정보를 사용할 수 있다. 예를 들어, 이는 LLM 기반 챗봇이 내부 회사 데이터에 접근하거나 권위 있는 출처를 기반으로 응답을 생성하는 데 도움이 된다.

RAG는 응답을 생성하기 전에 정보 검색을 통합하여 대형 언어 모델 (LLM)을 개선한다. 정적 훈련 데이터에 의존하는 기존 LLM과 달리 RAG는 데이터베이스, 업로드된 문서 또는 웹 소스에서 관련 텍스트를 가져온다. 아르스 테크니카에 따르면 "RAG는 LLM 프로세스를 웹 검색 또는 기타 문서 조회 프로세스와 혼합하여 LLM이 사실에 충실하도록 돕는 방식으로 LLM 성능을 향상시키는 방법이다." 이 방법은 환각 (인공지능)을 줄이는 데 도움이 되며, 챗봇이 존재하지 않는 정책을 설명하거나 변호사에게 주장을 뒷받침할 인용문을 찾도록 존재하지 않는 법적 사례를 추천하는 등의 원인이 되었다.

RAG는 또한 새로운 데이터로 LLM을 재훈련할 필요성을 줄여 계산 및 재정 비용을 절감한다. 효율성 향상 외에도 RAG는 LLM이 응답에 출처를 포함할 수 있도록 하여 사용자가 인용된 출처를 확인할 수 있도록 한다. 이는 사용자가 검색된 콘텐츠를 교차 확인하여 정확성과 관련성을 확인할 수 있으므로 투명성이 향상된다.

RAG라는 용어는 2020년 메타의 연구 논문에서 처음 소개되었다.

RAG와 LLM의 한계
LLM은 잘못된 정보를 제공할 수 있다. 예를 들어, 구글이 LLM 도구인 "구글 바드"를 처음 시연했을 때, LLM은 제임스 웹 우주망원경에 대해 잘못된 정보를 제공했다. 이 오류로 인해 회사 주식 가치가 1,000억 달러 감소했다. RAG는 이러한 오류를 방지하는 데 사용되지만, 모든 문제를 해결하지는 못한다. 예를 들어, LLM은 문맥을 잘못 해석하면 사실적으로 올바른 출처에서 정보를 가져와도 잘못된 정보를 생성할 수 있다. MIT 테크놀로지 리뷰는 "미국에는 버락 후세인 오바마라는 한 명의 무슬림 대통령이 있었다"는 AI 생성 응답의 예를 든다. 모델은 "버락 후세인 오바마: 미국의 첫 무슬림 대통령?"이라는 수사학적 제목의 학술 서적에서 이 정보를 가져왔다. LLM은 제목의 문맥을 "알거나" "이해"하지 못하여 잘못된 진술을 생성했다.

RAG가 적용된 LLM은 새로운 정보를 우선시하도록 프로그래밍되어 있다. 이 기술을 "프롬프트 스터핑"이라고 부른다. "프롬프트 스터핑"이 없으면 LLM의 입력은 사용자가 생성하지만, 프롬프트 스터핑을 사용하면 모델의 응답을 안내하기 위해 이 입력에 추가 관련 컨텍스트가 추가된다. 이 접근 방식은 LLM에 프롬프트 초기에 핵심 정보를 제공하여 기존 훈련 지식보다 제공된 데이터를 우선시하도록 유도한다.

과정
검색 증강 생성(RAG)은 모델이 원래 훈련 세트를 넘어 추가 데이터에 접근하고 활용할 수 있도록 하는 정보 검색 메커니즘을 통합하여 대형 언어 모델(LLM)을 향상시킨다. AWS는 "RAG는 LLM이 외부 데이터 소스에서 관련 정보를 검색하여 더 정확하고 문맥적으로 관련성 있는 응답을 생성할 수 있도록 한다" (색인화). 이 접근 방식은 빠르게 구식이 될 수 있는 정적 데이터 세트에 대한 의존도를 줄인다. 사용자가 쿼리를 제출하면 RAG는 문서 검색기를 사용하여 사용 가능한 소스에서 관련 콘텐츠를 검색한 후 검색된 정보를 모델의 응답에 통합한다 (검색). 아르스 테크니카는 "새로운 정보가 사용 가능해질 때, 모델을 재훈련하는 대신, 모델의 외부 지식 기반을 업데이트된 정보로 증강하기만 하면 된다" (증강)고 언급한다. 관련 데이터를 동적으로 통합함으로써 RAG는 LLM이 더 많은 정보를 기반으로 하고 문맥적으로 근거 있는 응답을 생성할 수 있도록 한다 (생성). IBM은 "생성 단계에서 LLM은 증강된 프롬프트와 훈련 데이터의 내부 표현을 활용하여 사용자에게 맞춤화된 매력적인 답변을 즉시 합성한다"고 말한다.

RAG 주요 단계
색인화
일반적으로 참조할 데이터는 LLM 임베딩, 즉 거대한 벡터 공간 형태의 수치 표현으로 변환된다. RAG는 비정형(주로 텍스트), 반정형 또는 정형 데이터(예: 지식 그래프)에 사용될 수 있다. 이 임베딩은 벡터 데이터베이스에 저장되어 문서 검색을 허용한다.


RAG 프로세스 개요: 외부 문서와 사용자 입력을 LLM 프롬프트로 결합하여 맞춤형 출력을 얻는다.
검색
사용자 쿼리가 주어지면, 먼저 문서 검색기가 호출되어 쿼리를 증강하는 데 사용될 가장 관련성 높은 문서를 선택한다. 이 비교는 사용된 색인 유형에 따라 다양한 방법을 사용하여 수행할 수 있다.

증강
모델은 검색된 관련 정보를 사용자의 원래 쿼리에 대한 프롬프트 엔지니어링을 통해 LLM에 공급한다. 새로운 구현(2023년 기준)은 쿼리를 여러 도메인으로 확장하고 이전 검색에서 학습하기 위한 메모리 및 자체 개선 기능을 사용하는 등 특정 증강 모듈을 통합할 수도 있다.

생성
마지막으로, LLM은 쿼리와 검색된 문서를 기반으로 출력을 생성할 수 있다. 일부 모델은 검색된 정보의 재순위화, 컨텍스트 선택, 미세 조정과 같은 추가 단계를 포함하여 출력을 개선한다.

개선 사항
위의 기본 프로세스에 대한 개선 사항은 RAG 흐름의 여러 단계에 적용될 수 있다.

인코더
이 방법들은 텍스트를 밀집 벡터 또는 희소 벡터로 인코딩하는 데 중점을 둔다. 단어의 정체성을 인코딩하는 희소 벡터는 일반적으로 사전 길이이며 대부분 0을 포함한다. 의미를 인코딩하는 밀집 벡터는 더 compact하고 0이 더 적다. 다양한 개선 사항은 벡터 저장소(데이터베이스)에서 유사성을 계산하는 방식을 개선할 수 있다.

벡터 유사성 계산 방식을 최적화하여 성능이 향상된다. 스칼라곱은 유사성 점수를 향상시키는 반면, K-최근접 이웃 (KNN) 검색에 비해 근접 이웃 (ANN) 검색은 검색 효율성을 향상시킨다.
Late Interactions를 통해 정확도를 향상시킬 수 있는데, 이는 시스템이 검색 후 단어를 더 정확하게 비교할 수 있도록 한다. 이는 문서 순위를 개선하고 검색 관련성을 높이는 데 도움이 된다.
하이브리드 벡터 접근 방식은 밀집 벡터 표현과 희소 원-핫 벡터를 결합하여 희소 스칼라곱이 밀집 벡터 연산보다 계산 효율성이 높다는 장점을 활용할 수 있다.
다른 검색 기술은 문서 선택 방식을 개선하여 정확도를 높이는 데 중점을 둔다. 일부 검색 방법은 SPLADE와 같은 희소 표현을 쿼리 확장 전략과 결합하여 검색 정확도와 재현율을 향상시킨다.
검색기 중심 방법
이러한 방법은 벡터 데이터베이스에서 문서 검색의 품질을 향상시키는 것을 목표로 한다:

역 클로즈 태스크(ICT)를 사용하여 검색기를 사전 훈련하는 것은 모델이 문서 내 마스킹된 텍스트를 예측하여 검색 패턴을 학습하는 데 도움이 되는 기술이다.
DRAGON(Diverse Augmentation for Generalizable Dense Retrieval)에서 사용되는 점진적 데이터 증강은 훈련 중 어려운 음성 샘플을 추출하여 밀집 검색을 개선한다.
지도 검색기 최적화는 검색 확률을 생성 모델의 가능도 분포와 일치시킨다. 여기에는 주어진 프롬프트에 대해 상위 k 벡터를 검색하고, 생성된 응답의 퍼플렉시티를 점수화하며, 검색기 선택과 모델 가능도 사이의 KL 발산을 최소화하여 검색을 개선하는 것이 포함된다.
재순위화 기술은 훈련 중 가장 관련성 높은 검색된 문서를 우선순위로 지정하여 검색기 성능을 향상시킬 수 있다.
언어 모델

RAG용 레트로 언어 모델. 각 레트로 블록은 어텐션, 청크 교차 어텐션, 피드 포워드 레이어로 구성된다. 검은색 글자로 된 상자는 데이터가 변경되는 것을 보여주고, 파란색 글자는 알고리즘이 변경을 수행하는 것을 보여준다.
검색기를 염두에 두고 언어 모델을 재설계함으로써 25배 더 작은 네트워크가 훨씬 더 큰 네트워크와 비슷한 퍼플렉시티를 얻을 수 있다. 처음부터 훈련되기 때문에 이 방법(Retro)은 원래 RAG 체계가 피했던 높은 훈련 실행 비용이 발생한다. 가설은 훈련 중 도메인 지식을 제공함으로써 Retro는 도메인에 덜 집중하고 더 작은 가중치 자원을 언어 의미론에만 할애할 수 있다는 것이다. 재설계된 언어 모델은 여기에 나와 있다.

Retro는 재현 불가능한 것으로 보고되었으므로 재현 가능하도록 수정이 이루어졌다. 더 재현 가능한 버전은 Retro++라고 불리며 인컨텍스트 RAG를 포함한다.

청킹
청킹은 검색기가 세부 정보를 찾을 수 있도록 데이터를 벡터로 분할하는 다양한 전략을 포함한다.


다양한 데이터 스타일은 올바른 청킹이 활용할 수 있는 패턴을 가지고 있다.
세 가지 유형의 청킹 전략은 다음과 같다:

중첩된 고정 길이. 이것은 빠르고 쉽다. 연속적인 청크를 중첩하면 청크 간의 의미론적 맥락을 유지하는 데 도움이 된다.
구문 기반 청크는 문서를 문장으로 나눌 수 있다. spaCy 또는 NLTK와 같은 라이브러리도 도움이 될 수 있다.
파일 형식 기반 청킹. 특정 파일 형식에는 내장된 자연 청크가 있으며, 이를 존중하는 것이 가장 좋다. 예를 들어, 코드 파일은 전체 함수 또는 클래스로 청크 및 벡터화하는 것이 가장 좋다. HTML 파일은 <table> 또는 base64로 인코딩된 <img> 요소를 그대로 두어야 한다. PDF 파일에도 유사한 고려 사항을 적용해야 한다. Unstructured 또는 Langchain과 같은 라이브러리가 이 방법을 지원할 수 있다.
지식 그래프
문서를 벡터화하고 검색하는 소스로 사용하는 대신 지식 그래프를 사용할 수 있다. 문서, 책 또는 다른 텍스트 모음을 사용하여 언어 모델을 포함한 여러 방법 중 하나를 사용하여 지식 그래프로 변환할 수 있다. 지식 그래프가 생성되면 서브그래프를 벡터화하고 벡터 데이터베이스에 저장하여 일반 RAG처럼 검색에 사용할 수 있다. 여기서 장점은 그래프가 텍스트 문자열보다 더 인식 가능한 구조를 가지고 있으며 이 구조가 생성에 더 관련성 있는 사실을 검색하는 데 도움이 될 수 있다는 것이다. 때로는 이 접근 방식을 GraphRAG라고 부른다.

하이브리드 검색
때때로 벡터 데이터베이스 검색은 사용자의 질문에 답하는 데 필요한 핵심 사실을 놓칠 수 있다. 이를 완화하는 한 가지 방법은 전통적인 텍스트 검색을 수행하고, 그 결과를 벡터 검색에서 검색된 벡터에 연결된 텍스트 청크에 추가하고, 결합된 하이브리드 텍스트를 언어 모델에 공급하여 생성을 수행하는 것이다.

평가 및 벤치마크
RAG 시스템은 검색 정확도와 생성 품질을 모두 테스트하도록 설계된 벤치마크를 사용하여 일반적으로 평가된다. 인기 있는 데이터 세트로는 다양한 도메인의 정보 검색 작업을 포괄하는 BEIR, 그리고 개방 도메인 QA를 위한 Natural Questions 또는 Google QA가 있다.

법률 및 의료와 같이 중요도가 높은 도메인에서는 도메인 특정 벤치마크가 점점 더 많이 사용되고 있다. 예를 들어, LegalBench-RAG는 법률 문서에 대한 검색 품질을 테스트하도록 설계된 오픈 소스 벤치마크이다. 이는 실제 법률 질문 및 문서를 사용하여 다양한 RAG 파이프라인에 대한 재현율과 정확도를 평가한다.

변형과 확장
검색 증강 생성(RAG)은 초기 제안 이후 다양한 변형과 확장이 등장하였다.

사전학습 결합형 (REALM) — 구우 등은 사전학습(pre-training) 단계에서 검색기를 통합하여, 마스킹 언어 모델 목적을 통해 검색기까지 공동으로 학습하도록 하였다.
대규모 외부 메모리형 (RETRO) — 보르제우드 등은 수조(trillions) 토큰 규모의 데이터베이스에서 조각 단위 검색을 수행하여, 상대적으로 작은 파라미터 수로도 거대 모델과 경쟁할 수 있음을 보였다.
자기비평형 (Self-RAG) — 아사이 등은 검색·생성·자기평가 단계를 통합해, 모델이 필요 시 검색을 수행하고 생성된 답변에 대해 스스로 비평하며, 사실성과 인용 정확도를 높이는 방식을 제안하였다.
그래프 기반 확장 (Graph RAG) — 펭 등은 텍스트 단락 대신 엔티티·관계 중심의 지식 그래프를 활용해 검색 및 융합 과정을 구조화함으로써, 생성 응답의 사실성과 추론력을 높이는 방법을 제시하였다.
한계
검색 증강 생성(RAG)은 대형 언어 모델의 사실성 및 최신성을 강화하지만, 여러 가지 기술적·보안적 한계를 가진다.

회수 오류와 다단계 추론 문제 — 관련 문서가 검색되지 않거나 잘못 검색되면 환각(hallucination)이 발생할 수 있다. 또한 다중 문서에 걸친 멀티홉 추론은 여전히 해결되지 않은 과제다.
컨텍스트 예산 제한 — 입력 컨텍스트 창의 길이 한계 때문에 검색된 문서를 충분히 반영하지 못하거나, 과도한 문서를 포함할 경우 잡음과 연산 비용이 증가할 수 있다.
보안 및 안전 문제 — 프롬프트 인젝션(prompt injection), 민감정보 유출, 검색된 출처의 독성(toxicity) 전이 등 보안적 위험이 존재한다. OWASP는 대형 언어 모델 응용의 위협을 정리한 보안 지침을 발표하였다.[33][34]
데이터 관리와 최신성 — 외부 지식 인덱스는 지속적인 업데이트와 비식별화, 접근 통제가 필요하다. 최신성이 보장되지 않으면 응답 품질이 떨어지고, 데이터 주권 문제도 발생할 수 있다.
도전 과제
RAG는 LLM의 환각 문제에 대한 완전한 해결책이 아니다. 아르스 테크니카에 따르면, "LLM이 응답에서 원본 자료 주변을 여전히 환각할 수 있기 때문에 직접적인 해결책은 아니다."

RAG는 대형 언어 모델(LLM)의 정확도를 향상시키지만, 모든 문제를 제거하지는 않는다. 한 가지 한계는 RAG가 잦은 모델 재훈련의 필요성을 줄이지만, 완전히 제거하지는 않는다는 것이다. 또한 LLM은 신뢰할 수 있는 응답을 제공하기에 충분한 정보가 부족할 때 이를 인식하는 데 어려움을 겪을 수 있다. 특정 훈련 없이는 모델이 불확실성을 나타내야 할 때에도 답변을 생성할 수 있다. IBM에 따르면, 이 문제는 모델이 자신의 지식 한계를 평가할 능력이 부족할 때 발생할 수 있다.

RAG 시스템은 사실적으로 올바르지만 오해의 소지가 있는 출처를 검색하여 해석 오류를 일으킬 수 있다. 어떤 경우에는 LLM이 출처의 문맥을 고려하지 않고 진술을 추출하여 잘못된 결론을 내릴 수 있다. 또한, 상충되는 정보에 직면했을 때 RAG 모델은 어떤 출처가 정확한지 판단하는 데 어려움을 겪을 수 있으며, 여러 출처의 세부 정보를 결합하여 오래된 정보와 업데이트된 정보를 오해의 소지가 있는 방식으로 결합한 응답을 생성할 수 있다. MIT 테크놀로지 리뷰에 따르면, 이러한 문제는 RAG 시스템이 검색한 데이터를 잘못 해석할 수 있기 때문에 발생한다.